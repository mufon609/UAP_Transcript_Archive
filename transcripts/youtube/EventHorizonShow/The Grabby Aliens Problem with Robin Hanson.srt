1
00:00:00,000 --> 00:00:11,000
You have fallen into Event Horizon with John Michael Gaudier.

2
00:00:30,000 --> 00:00:37,000
Robin Hansen joins John today to discuss David Kipping's critique of the Grabby Aliens Hypothesis.

3
00:00:37,000 --> 00:00:42,000
We recommend you watch David's video over on his own channel, Cool Worlds.

4
00:00:42,000 --> 00:00:46,000
As always, there's a link in the description and the pinned comment below.

5
00:00:46,000 --> 00:00:55,000
Robin Hansen is an Associate Professor of Economics and received his PhD in 1997 in Social Sciences from Caltech.

6
00:00:55,000 --> 00:01:04,000
He joined George Mason's Economics Faculty in 1999 after completing a two-year postdoc at UC Berkeley.

7
00:01:04,000 --> 00:01:11,000
His major fields of interest include health policy, regulation, and formal political theory.

8
00:01:11,000 --> 00:01:13,000
Robin Hansen, welcome back to the program.

9
00:01:13,000 --> 00:01:15,000
Great to be here. What should we talk about?

10
00:01:15,000 --> 00:01:17,000
We can talk about anything. We can talk about anything.

11
00:01:17,000 --> 00:01:18,000
Okay.

12
00:01:19,000 --> 00:01:22,000
Actually, I have my favorite topic.

13
00:01:22,000 --> 00:01:30,000
And you were the person that first coined the term, the great filter, or more accurately filters, perhaps,

14
00:01:30,000 --> 00:01:36,000
that there is something or a series of things that prevent intelligent life from being common.

15
00:01:36,000 --> 00:01:41,000
And it's very easy to back this up because take a look at the universe, we don't see anything.

16
00:01:41,000 --> 00:01:45,000
We don't see another one, grabby aliens. We don't see them.

17
00:01:45,000 --> 00:01:50,000
You know, we just simply see nature. And everywhere we look, that nature is all the same.

18
00:01:50,000 --> 00:01:56,000
So no one appears to be altering their environment out there, so far as we can tell.

19
00:01:56,000 --> 00:02:04,000
Now, tell us about the idea of the filters and what led you to formulate that?

20
00:02:04,000 --> 00:02:11,000
Well, it was a long time ago, the late 1990s, and I was a new grad student, actually, an older returning student.

21
00:02:11,000 --> 00:02:15,000
But I find it hard to resist these really basic big questions.

22
00:02:15,000 --> 00:02:19,000
I find that most academics shy away from them for some reason.

23
00:02:19,000 --> 00:02:22,000
They just seem too hard or maybe too presumptuous.

24
00:02:22,000 --> 00:02:25,000
I arrogantly, I guess, jump in.

25
00:02:25,000 --> 00:02:31,000
So there's the great Fermi's question, if you will, why does the universe look empty?

26
00:02:31,000 --> 00:02:35,000
And I decided to reframe that in terms of the filter concept.

27
00:02:35,000 --> 00:02:45,000
That is, a non-empty universe would be a big visible alien civilization that arose somehow from an earlier point in time when it was simple dead matter.

28
00:02:45,000 --> 00:02:51,000
And there was some path that history went along from simple dead matter to that alien civilization.

29
00:02:51,000 --> 00:03:02,000
And the idea is that path must be really hard because we see a lot of simple dead matter and we see almost nothing, basically only maybe us, that reaches the end of that path.

30
00:03:02,000 --> 00:03:06,000
So the idea of the great filter is that's hard. Something along that path must be hard.

31
00:03:06,000 --> 00:03:10,000
Well, I can think of a lot of candidates, you know, there's a bunch of them.

32
00:03:10,000 --> 00:03:14,000
And I think the one that bothers me the most that I think about the most is water.

33
00:03:14,000 --> 00:03:21,000
If you don't have any land, how are you going to master fire and do all of these things that you need to have a civilization?

34
00:03:21,000 --> 00:03:26,000
It doesn't preclude intelligence, but you could see an octopus looks fairly intelligent.

35
00:03:26,000 --> 00:03:29,000
I mean, it's not a human, but it's not going to smelt iron.

36
00:03:29,000 --> 00:03:31,000
I don't know that we know that.

37
00:03:31,000 --> 00:03:40,000
So the key thing to know about history is we have this collapsed version where we focus much more on the recent history than the distant history.

38
00:03:40,000 --> 00:03:46,000
And events lately have just been happening crazy fast compared to events a long time ago.

39
00:03:46,000 --> 00:03:58,000
So the simplest model of things being hard is where I call it a tri-tri step where something just needs to keep searching in a vast space until it finally stumbles upon the right solution.

40
00:03:58,000 --> 00:04:08,000
And if in history things were hard in the sense of tri-tri steps, it turns out that those steps should have been roughly equally spaced in time in terms of achieving them.

41
00:04:08,000 --> 00:04:14,000
And that means that anything that's been happening really fast lately couldn't have been that hard.

42
00:04:14,000 --> 00:04:20,000
So we've done amazing things in the last, say, 10,000 or a million years, and we should be proud of those.

43
00:04:20,000 --> 00:04:30,000
But because they've happened so rapidly, it's really hard to see them as things that would have taken billions of years to take place if we had just kept trying.

44
00:04:30,000 --> 00:04:42,000
So we could have a what I call a tri-one step, some way in which at a certain crucial point we had to stumble in a certain direction as opposed to another direction, and we could never undo it.

45
00:04:42,000 --> 00:04:47,000
Once you stumbled in the wrong direction, it was just done and you were never going to be able to get out of it.

46
00:04:47,000 --> 00:05:06,000
And we could have had some difficult steps like that in our recent history, ways in which our social organization or organization of our brain or something happened to stumble upon a certain kind of organization that if we had gone another direction, we would just get stuck there and never be able to get out of it.

47
00:05:06,000 --> 00:05:16,000
But it doesn't look like in the last million years we could have had tri-tri steps where it would take on average billions of years to accomplish.

48
00:05:16,000 --> 00:05:26,000
What seems to have taken billions of years is to get to that stage, to where you have something that is capable of creating civilization as we have now.

49
00:05:26,000 --> 00:05:30,000
And then everything happens fast, you know, just within a...

50
00:05:30,000 --> 00:05:36,000
Right. But if you're thinking about the ocean, you were saying, you know, imagine we reached our level a million years ago and we were under the water.

51
00:05:36,000 --> 00:05:39,000
Could we, you know, do industry or something?

52
00:05:39,000 --> 00:05:52,000
Well, it might take a lot longer, but since we did it so crazy fast, I don't feel very confident in thinking an under ocean civilization couldn't have done it in maybe 10 million years instead of 10,000.

53
00:05:52,000 --> 00:05:55,000
And still that's geologically a very short amount of time.

54
00:05:55,000 --> 00:05:57,000
So you do have to wonder.

55
00:05:57,000 --> 00:06:01,000
The question, though, I mean, you know, what bothers me about it is physiology.

56
00:06:01,000 --> 00:06:07,000
I mean, could a fish, an intelligent fish, work out a way, you know, to build technology?

57
00:06:07,000 --> 00:06:11,000
Maybe. Maybe if it did it long enough.

58
00:06:11,000 --> 00:06:13,000
But I would imagine it's a lot harder.

59
00:06:13,000 --> 00:06:23,000
And I'm not so sure what the timeframes are, because in our 4.5 billion year time frame, we're recent, you know, and there wasn't anything else that we've ever seen evidence for.

60
00:06:23,000 --> 00:06:27,000
Just dinosaurs and trilobites and things like that.

61
00:06:27,000 --> 00:06:33,000
So that seems to suggest that to get to that stage takes a long time, at least based on Earth.

62
00:06:33,000 --> 00:06:36,000
But you can also play with ideas that it could happen much faster.

63
00:06:36,000 --> 00:06:44,000
So do you think that that this process could have been sped up on Earth if the different circumstances had presented themselves?

64
00:06:44,000 --> 00:06:49,000
Well, so again, it depends on the kind of thing that was happening in the past.

65
00:06:49,000 --> 00:06:51,000
So there are these simpler models.

66
00:06:51,000 --> 00:06:57,000
Now, one kind of a model is just a process with a delay, a fuse, basically.

67
00:06:57,000 --> 00:07:05,000
Something just had to step through a sequence of steps that each takes a certain amount of time until you get to the final spot.

68
00:07:05,000 --> 00:07:08,000
Now, for that, it might depend like what the rate depends on.

69
00:07:08,000 --> 00:07:17,000
So say there could be a planet metabolism parameter, say, and then a high metabolism planet might go through those steps faster than a low metabolism planet.

70
00:07:17,000 --> 00:07:24,000
And then, you know, the rate of planets might depend on that metabolism parameter, which might, say, depend on a volume parameter.

71
00:07:24,000 --> 00:07:26,000
What's the volume of biomass on the planet?

72
00:07:26,000 --> 00:07:35,000
So I could see steps like that varying with the volume of biomass and the metabolism rate of the biomass, things like that.

73
00:07:35,000 --> 00:07:41,000
That's a way in which even a delay step could depend on context like that.

74
00:07:41,000 --> 00:07:48,000
But if we're talking like a tri-tri step where it just has to search and finally find something at a random luck,

75
00:07:48,000 --> 00:07:55,000
then the rate at which you find the constant, the expected time to achieve it might vary, say, on the size or metabolism.

76
00:07:55,000 --> 00:08:04,000
But we have the statistical analysis that conditional on achieving success within a particular lucky time period,

77
00:08:04,000 --> 00:08:10,000
then the expected, the actual duration of each step would be roughly equally distributed in time and history.

78
00:08:10,000 --> 00:08:17,000
So the best candidates for those things are the things that seem to have taken maybe a half billion years or a billion years in our history.

79
00:08:17,000 --> 00:08:25,000
And we have some candidates for those, you know, multicellularity, eukaryotes, even just being able to process sunlight.

80
00:08:26,000 --> 00:08:29,000
I do sometimes wonder, though, if those can be short-circuited.

81
00:08:29,000 --> 00:08:35,000
I mean, a lot is made of the prokaryotic to eukaryotic leap, of course, and that's just a pretty good candidate.

82
00:08:35,000 --> 00:08:43,000
But at the same time, there's been recent work that suggests, well, maybe it's actually not really so much for leap, but a matter of symbiosis that might have happened a lot earlier.

83
00:08:44,000 --> 00:08:49,000
It was just by chance that it took as long as it did, in which case that changes the time frames.

84
00:08:49,000 --> 00:08:57,000
And you don't end up with a planet like Earth where we appear at almost the end, you know, almost the end of its habitability.

85
00:08:57,000 --> 00:09:07,000
Here we are. And if that's a constant for the universe, I mean, that's that in itself is a solution to the front paradox.

86
00:09:08,000 --> 00:09:14,000
Intelligence is very rare and only exists for a very short time because it shows up right about when its planet is about to die.

87
00:09:14,000 --> 00:09:22,000
That, of course, doesn't really account so much for M dwarfs and type K dwarfs, but M dwarfs aren't faring very well as far as habitability right now.

88
00:09:22,000 --> 00:09:28,000
And that just would leave type K to have a longer chance than the sun had. What are your views on that?

89
00:09:28,000 --> 00:09:32,000
I think it depends, again, a lot on the viability of the ocean worlds.

90
00:09:32,000 --> 00:09:38,000
Most of the problems with small stars are for the land.

91
00:09:38,000 --> 00:09:45,000
There's, you know, problem of the flares and the problem of tidal locking and the lack of, say, continental drift.

92
00:09:45,000 --> 00:09:51,000
Those are all issues for the land, but ocean worlds don't seem to suffer that much from those problems.

93
00:09:51,000 --> 00:09:59,000
So it's more that ocean worlds are viable for advanced intelligence, the less the small more that the smaller stars seem viable.

94
00:09:59,000 --> 00:10:03,000
Now, what about being locked in by gravity on like a super earth?

95
00:10:03,000 --> 00:10:08,000
And there are some models that show that a majority of super earths probably are going to be water worlds like that.

96
00:10:08,000 --> 00:10:13,000
But at the same time, they're going to be, you know, say, two Earth masses, which I think is the upper limit.

97
00:10:13,000 --> 00:10:17,000
They think that life could exist on easily anyway.

98
00:10:17,000 --> 00:10:23,000
That's awfully heavy if you want to go out and be a grabby alien and start launching rockets off it.

99
00:10:23,000 --> 00:10:35,000
Right. But I mean, anything that we did within a few hundred years of it being even remotely possible just can't count as obviously hard in the cosmological scheme of things.

100
00:10:35,000 --> 00:10:42,000
So it might be much harder in another context, but that's still different from being fundamentally hard.

101
00:10:42,000 --> 00:10:55,000
So, you know, when we're looking at the great filters, say we're looking at the fact that we have roughly 10 to the 24 stars in the observable universe, none of which seem to be grabby.

102
00:10:55,000 --> 00:11:00,000
So we're looking for filters at that scale over billions of years.

103
00:11:00,000 --> 00:11:03,000
So we're really looking for huge filters.

104
00:11:03,000 --> 00:11:10,000
So something that took a few hundred years in our history on that basis isn't very plausible as a candidate.

105
00:11:10,000 --> 00:11:15,000
We need things that would take expected billions or trillions of years to be hard steps.

106
00:11:15,000 --> 00:11:21,000
And so then the question is, imagine an advanced civilization like us stuck under the ocean on a heavy planet.

107
00:11:21,000 --> 00:11:25,000
OK, yes, it'll take them longer, but will it take them trillions of years?

108
00:11:25,000 --> 00:11:33,000
Probably not. Probably not. If there is indeed a physical way to accomplish it one way or another, and then it isn't hard.

109
00:11:33,000 --> 00:11:40,000
It's just you're waiting, even if waiting is just waiting for a grabby alien civilization to come along and pull you out of your ocean.

110
00:11:40,000 --> 00:11:47,000
Maybe they just build ladders or, you know, maybe there's lots of possible physics solutions, right?

111
00:11:47,000 --> 00:11:52,000
Big rockets, big rockets. I mean, if you've got, you know, so there are solutions.

112
00:11:52,000 --> 00:12:00,000
And then there are certain things like space elevators and all that that might serve them better than, you know, big space rockets and things like that.

113
00:12:00,000 --> 00:12:03,000
Now, hard steps. And this is a related idea.

114
00:12:03,000 --> 00:12:12,000
The hard steps that you have to go through these doors and it doesn't seem likely that a lot of civilizations are going to make it through all of them.

115
00:12:12,000 --> 00:12:16,000
I think it was calculated at six. What's your view on that?

116
00:12:16,000 --> 00:12:20,000
And more importantly, is grabby aliens actually predicated on that?

117
00:12:20,000 --> 00:12:32,000
The I mean, I've seen a couple of papers lately and videos which have challenged either the hard steps view or the grabby aliens view.

118
00:12:32,000 --> 00:12:38,000
And basically, some of them are saying there are no hard steps in our history.

119
00:12:38,000 --> 00:12:53,000
So there are many possible kinds of steps we could have had in our history, including delays and and maybe waiting for some other event to be ready or try one step or try, try step.

120
00:12:53,000 --> 00:13:01,000
And it's the try, try steps that we call the hard steps if they have an expected duration that's really long, like, say, many billions of years.

121
00:13:01,000 --> 00:13:08,000
So lots of stuff probably happened in our history, but were any of them try, try steps is the key question.

122
00:13:08,000 --> 00:13:21,000
So if none of them were try, try steps, you see, then the probability of reaching our level here could be pretty high unless there were some really hard try one steps.

123
00:13:21,000 --> 00:13:28,000
And, you know, that's not crazy. I tend to think that there are just so many possible ways our history could have.

124
00:13:28,000 --> 00:13:36,000
Again, the chance that no none of our past would be attributed to a try, try step seems unlikely, but it's not crazy.

125
00:13:36,000 --> 00:13:43,000
But nevertheless, I would focus on the key idea that you look out in the universe and you see nothing.

126
00:13:43,000 --> 00:13:46,000
So clearly something has to be really hard here.

127
00:13:47,000 --> 00:13:54,000
A key question is how hard will it be to go from we are now into this being a visible civilization?

128
00:13:54,000 --> 00:14:01,000
At that point, actually, I find it that hard to believe that there's try, try steps in our future.

129
00:14:01,000 --> 00:14:08,000
That is, I can see maybe, you know, try one steps, we kill ourselves or some delay steps.

130
00:14:08,000 --> 00:14:17,000
But, you know, try, try stuff that would be of the same difficulty as anyone in our past would be something that takes expected billions of years to complete.

131
00:14:17,000 --> 00:14:23,000
And I just find it really hard to believe that even where we are now, it would take billions of years to actually expand to the universe.

132
00:14:23,000 --> 00:14:27,000
Seems like we could do that at most in a million years.

133
00:14:27,000 --> 00:14:33,000
Well, if you've got a million years, even at sublight speeds, you've got the galaxy, essentially, as long as you're willing to go slow.

134
00:14:33,000 --> 00:14:35,000
Right. I'm just talking about when you even start the process.

135
00:14:35,000 --> 00:14:37,000
Could we even get started in a million years?

136
00:14:37,000 --> 00:14:39,000
I mean, yeah, whenever you start.

137
00:14:39,000 --> 00:14:44,000
Well, would you say that we are now at the very start of becoming grabbing?

138
00:14:44,000 --> 00:14:48,000
Well, we are clearly within view in some sense of the possibility.

139
00:14:48,000 --> 00:14:50,000
We can't do it now.

140
00:14:50,000 --> 00:14:56,000
But, I mean, by some estimations, it might possibly something we could do within a thousand years, maybe even shorter.

141
00:14:56,000 --> 00:14:57,000
Possibly so.

142
00:14:57,000 --> 00:15:01,000
A thousand years is just crazy short time on a cosmological scale.

143
00:15:01,000 --> 00:15:03,000
So it's insanely, insanely short.

144
00:15:03,000 --> 00:15:06,000
But look what we've done over the last thousand years.

145
00:15:06,000 --> 00:15:11,000
I mean, we, you know, grab a cell phone, look at that and then say, what did they have a thousand years ago?

146
00:15:11,000 --> 00:15:17,000
So it seems to me like I think within a million years, will we either be grabby or we never will.

147
00:15:17,000 --> 00:15:20,000
Like by killing ourselves or something.

148
00:15:20,000 --> 00:15:26,000
And so that means this next step between now and becoming grabby is it's not at all certain.

149
00:15:26,000 --> 00:15:31,000
But whatever is the difficulty, it isn't try, try, step with an expected time of billions of years.

150
00:15:31,000 --> 00:15:38,000
Do you see a point where a civilization can actually become so grabby that it goes to a nearby galaxy?

151
00:15:38,000 --> 00:15:43,000
It goes through the trouble of crossing intergalactic space and look by whatever means.

152
00:15:43,000 --> 00:15:53,000
In fact, our analysis of grabby alien civilizations just makes the very simplest model that once they start expanding, they expand at a speed and they just don't stop.

153
00:15:53,000 --> 00:15:55,000
And I think that's roughly right.

154
00:15:55,000 --> 00:16:01,000
That is once they got into the ability to expand, I don't think the gap between galaxies would be an obstacle.

155
00:16:01,000 --> 00:16:03,000
So they would just keep expanding.

156
00:16:03,000 --> 00:16:08,000
That doesn't look like there's that there's maybe, you know, dragons between stars that are hard to get through.

157
00:16:08,000 --> 00:16:15,000
But whatever there are between stars, that's probably no easier than harder than getting between galaxies.

158
00:16:15,000 --> 00:16:19,000
That is, it seems like if you just drift long enough, go far enough, you'll get between them.

159
00:16:19,000 --> 00:16:23,000
Well, and it also has to be said that empty intergalactic space is not empty.

160
00:16:23,000 --> 00:16:26,000
There are stars out there that are just not organized in the galaxies.

161
00:16:26,000 --> 00:16:28,000
There's just, you know, ones that have been ejected.

162
00:16:28,000 --> 00:16:29,000
Right.

163
00:16:29,000 --> 00:16:34,000
But your expected chance of hitting one or something near it as you're flying through is going to be really small.

164
00:16:34,000 --> 00:16:35,000
Unless you know about it.

165
00:16:35,000 --> 00:16:36,000
Yeah.

166
00:16:36,000 --> 00:16:37,000
Right.

167
00:16:37,000 --> 00:16:38,000
It's going to be very small.

168
00:16:38,000 --> 00:16:41,000
But if you know about it and you're like, well, there's a there's a gas station.

169
00:16:41,000 --> 00:16:43,000
Yeah.

170
00:16:43,000 --> 00:16:46,000
Now, in regards to now, there's been a recent video by Dr.

171
00:16:46,000 --> 00:16:49,000
David Kipping that offers a counterpoint to this.

172
00:16:49,000 --> 00:16:50,000
What are your thoughts?

173
00:16:50,000 --> 00:16:51,000
Thoughts on that?

174
00:16:51,000 --> 00:17:05,000
On his view of gravity aliens, his paper or his video and another paper, another two papers I saw recently all seem to share this feature that they think it's really unlikely that we now would ever become gravity.

175
00:17:05,000 --> 00:17:13,000
So they even think like maybe on Earth it was more likely than not that someone like us would evolve on a planet like Earth.

176
00:17:13,000 --> 00:17:29,000
And if it's really likely that we would have evolved here and yet none of the 10 to the 24 stars we see have gone grabby, then you have to be estimating a really low chance, not just 1% or something.

177
00:17:29,000 --> 00:17:34,000
You mean like one part in 10 to the 20 chance of us becoming gravity.

178
00:17:34,000 --> 00:17:37,000
And that's just really hard for me to believe.

179
00:17:37,000 --> 00:17:43,000
I mean, I don't think it's obvious we will, but I think we have a least one in a million.

180
00:17:43,000 --> 00:17:44,000
I think it's better than that.

181
00:17:44,000 --> 00:17:47,000
I mean, look, this planet.

182
00:17:47,000 --> 00:17:48,000
Yes.

183
00:17:48,000 --> 00:17:53,000
But all I need is to say if there's at least a one in a million chance, then that's enough.

184
00:17:53,000 --> 00:17:58,000
That's in conflict with these analyses that need to assume a much, much lower chance.

185
00:17:58,000 --> 00:18:05,000
It is. And then eventually there's being forced to be grabby, eventually, because, you know, Earth's time is limited as far as habitability.

186
00:18:05,000 --> 00:18:09,000
So we're going to have to go somewhere else if we're still here or die, which I don't think we will.

187
00:18:09,000 --> 00:18:16,000
I think we probably if we are capable of it, we'll head out to Europa or Titan or just follow the wave out.

188
00:18:16,000 --> 00:18:24,000
And eventually got to move to the next star system, although I suspect we'll probably have done that a long time before then.

189
00:18:24,000 --> 00:18:34,000
But if we hadn't, eventually you're forced to because you have to find a new home eventually unless you can eke out some kind of existence around a white dwarf on cinder planets.

190
00:18:34,000 --> 00:18:40,000
So almost seems like eventually you have to become grabby if you're long lived.

191
00:18:40,000 --> 00:18:43,000
Would you agree with that or would you throw in pepper that with caveats?

192
00:18:43,000 --> 00:18:50,000
So we're mainly focused here on the idea that there's a least a small chance.

193
00:18:50,000 --> 00:18:59,000
So if we say there's at least a small chance, then that is in conflict with these other models that require a really crazy low chance.

194
00:18:59,000 --> 00:19:05,000
So for the purpose of talking about those models, all we have to do is argue about is there at least a small chance.

195
00:19:05,000 --> 00:19:11,000
So for that, we don't need to talk about a typical case or what would motivate us, perhaps typically.

196
00:19:11,000 --> 00:19:15,000
We just need in a small fraction of cases that happens.

197
00:19:15,000 --> 00:19:24,000
So the arguments against that need to put some sort of barrier, sort of pretty universal block that would prevent it.

198
00:19:24,000 --> 00:19:28,000
They can't just talk about typical cases, right?

199
00:19:28,000 --> 00:19:36,000
But if I'm if I'm thinking about how big is the filter in front of us, I have to think about what's the most likely thing that will prevent us from expanding.

200
00:19:36,000 --> 00:19:39,000
The thing that most easily comes to people's minds is going extinct.

201
00:19:39,000 --> 00:19:42,000
We just completely destroy ourselves.

202
00:19:42,000 --> 00:19:45,000
When I think about that, that's actually pretty hard to do.

203
00:19:45,000 --> 00:19:51,000
Even a worldwide nuclear war wouldn't do it, wouldn't kill all humans.

204
00:19:51,000 --> 00:19:53,000
Oh, it's very, very hard. Right.

205
00:19:53,000 --> 00:19:55,000
It is very hard to go to extinct.

206
00:19:55,000 --> 00:20:00,000
And I think that that's not stated enough in this day and age that you write a nuclear war will not do it.

207
00:20:00,000 --> 00:20:03,000
Right. Rapid, drastic climate change isn't going to do it.

208
00:20:03,000 --> 00:20:07,000
There's just not very many things that can cause human extinction.

209
00:20:07,000 --> 00:20:12,000
And if you get multiple planet or multiple star systems, you get grabby.

210
00:20:12,000 --> 00:20:14,000
You are not going extinct.

211
00:20:14,000 --> 00:20:17,000
So certainly spreading across multiple star systems will make it much harder.

212
00:20:17,000 --> 00:20:28,000
So then, I mean, at least to go extinct from a war or even a pandemic or something or a resource, you know, or, you know, asteroid or supernova or things like that.

213
00:20:28,000 --> 00:20:38,000
But I actually think the most likely scenario by which we would not become grabby is, in fact, a world government that chooses not to.

214
00:20:38,000 --> 00:20:40,000
Oh, that certainly could happen.

215
00:20:40,000 --> 00:20:48,000
Right. That is, I actually think I see now when people talk now about the possibility of interstellar colonization, a lot of people don't like the idea.

216
00:20:48,000 --> 00:20:52,000
And the reason they don't like it makes some sense.

217
00:20:52,000 --> 00:20:58,000
So, you know, up until recently, the world was full of conflicting empires.

218
00:20:58,000 --> 00:21:08,000
Right. People fighting fiercely often and not agreeing on things and not being able to enforce any civilization level rules.

219
00:21:08,000 --> 00:21:17,000
And then in the last century, we have formed a global elite culture with a set of sort of global rules that we enforce together.

220
00:21:17,000 --> 00:21:31,000
And that's allowed us to have an integrated society, to have a more peaceful society, to have more gains from trade across the whole world and to share a culture that we can enforce.

221
00:21:31,000 --> 00:21:37,000
And part of that will be in the future not letting people get too strange if we don't want them to.

222
00:21:37,000 --> 00:21:41,000
And I think that will be more of an issue in the coming centuries.

223
00:21:41,000 --> 00:21:55,000
People will like being part of a shared civilization that shares morality rules and behavior rules and enforces them and successfully coordinates to keep a peace and to keep rules and to prevent strangeness.

224
00:21:55,000 --> 00:21:57,000
Which doesn't sound that bad.

225
00:21:57,000 --> 00:22:06,000
Right. But then they'll realize the moment anyone's allowed to leave the solar system and start colonizing another star, that's the end of that era.

226
00:22:06,000 --> 00:22:09,000
Those colonists will be able to change without limit.

227
00:22:09,000 --> 00:22:20,000
Their descendants could go off in all sorts of directions and change themselves and have different moral rules and become powerful and then come back and contest or control with here.

228
00:22:20,000 --> 00:22:26,000
Letting any colonists leave is, in fact, the end of a coherent civilization.

229
00:22:26,000 --> 00:22:31,000
It's the era beginning the endless era of competition and change.

230
00:22:31,000 --> 00:22:36,000
And that could scare people and put people off and they might just say, no, we don't want that.

231
00:22:36,000 --> 00:22:47,000
We'd rather just all stay around here and be together and get along and be able to enforce our rules on each other and be at peace with each other.

232
00:22:47,000 --> 00:22:56,000
All right. Now you could call that, let's call it Hanson's Paradise Solution to the Fermi Paradox in that potentially one solution is quite simply.

233
00:22:56,000 --> 00:23:02,000
Everybody ends up turning their planet into a paradise and nobody wants to leave it and it would be silly and dangerous if they did.

234
00:23:02,000 --> 00:23:06,000
Right. Well, it's the everyone problem, right?

235
00:23:06,000 --> 00:23:11,000
I mean, if I said what's the most likely thing that prevents us from expanding, that's my best guess.

236
00:23:11,000 --> 00:23:16,000
But I can't put that at more than a 99 percent chance of happening for us.

237
00:23:16,000 --> 00:23:25,000
So I got to leave at least a one percent chance that we go out and expand and have forever more conflict and change.

238
00:23:25,000 --> 00:23:34,000
So that means, you know, this is maybe a one percent part of the great filter, a factor of 10 to the two out of 10 to the 24.

239
00:23:34,000 --> 00:23:36,000
Still only a tiny part of the filter.

240
00:23:36,000 --> 00:23:42,000
And so we still face the question of explaining why the rest of the universe looks so empty.

241
00:23:42,000 --> 00:23:44,000
This can only be a small part of the explanation.

242
00:23:44,000 --> 00:23:51,000
I guess you could go the opposite direction, go really dark with us and that, you know, might it also be possible with a tyranny?

243
00:23:51,000 --> 00:23:55,000
In other words, a tyrannical government that says no one leaves.

244
00:23:55,000 --> 00:23:58,000
You are all stuck here no matter what.

245
00:23:58,000 --> 00:24:03,000
Well, that's pretty much the same scenario, modulo, some marketing descriptions.

246
00:24:03,000 --> 00:24:09,000
Of course, a tyranny would try to present it in everyone's best interest.

247
00:24:09,000 --> 00:24:13,000
Do you think a civilization can become too comfortable to become grabby?

248
00:24:13,000 --> 00:24:17,000
In other words, I'm not talking about a utopia like this.

249
00:24:17,000 --> 00:24:23,000
I'm talking or a tyranny. I'm talking about people just say intelligence evolves out.

250
00:24:23,000 --> 00:24:26,000
In other words, you no longer need intelligence.

251
00:24:26,000 --> 00:24:38,000
So you just sort of become no longer capable of continuing the building of technology in order to achieve grabbiness and that you just end up dumbed down for lack of a better term.

252
00:24:38,000 --> 00:24:44,000
I would think of it more in terms of internal levels and types of competition within our civilization.

253
00:24:44,000 --> 00:24:51,000
So in the past, we had fierce competition, but we had very low levels of technology and very slow rates of innovation.

254
00:24:51,000 --> 00:24:55,000
Not because people wouldn't have wanted to. They just didn't know how.

255
00:24:55,000 --> 00:25:06,000
So in the future, we might not have much change or risk taking that would require either little competition.

256
00:25:06,000 --> 00:25:14,000
That is, people weren't really forced to try new things or to take chances because they weren't threatened much.

257
00:25:14,000 --> 00:25:20,000
Or somehow lost the ability to innovate and be technical that we somehow couldn't get back again.

258
00:25:20,000 --> 00:25:23,000
I guess both are in principle possible.

259
00:25:23,000 --> 00:25:28,000
But look, even in the past, when we have very low rates of innovation, it was still pretty consistent.

260
00:25:28,000 --> 00:25:34,000
So like foragers for a million years, roughly doubled every quarter million years.

261
00:25:34,000 --> 00:25:36,000
So it's a very slow rate, but it's still a rate.

262
00:25:37,000 --> 00:25:44,000
And then the farming area doubled roughly every thousand years, a rate 250 times faster, but still much lower than our world today.

263
00:25:44,000 --> 00:25:54,000
So even if we lost the ability to have rapid technical change, that's still not the same as losing the ability to have any technical change.

264
00:25:54,000 --> 00:25:59,000
And even doubling every thousand years for a million years still gets you enormous distances.

265
00:25:59,000 --> 00:26:10,000
So I would have to say unless we have a crazy, crazy drastic reduction in our ability to innovate, the main thing would have to be a loss of competition.

266
00:26:10,000 --> 00:26:20,000
A way in which there's a central government that says, no, you can't compete in this way or way everybody is somehow fat and happy and we don't allow variations to compete with that.

267
00:26:20,000 --> 00:26:23,000
And you also have to wonder about technology because if you...

268
00:26:23,000 --> 00:26:30,000
Sorry, I'm an apocalyptic science fiction author, this is where I started and I always think of the really bad things.

269
00:26:30,000 --> 00:26:42,000
But as we develop artificial intelligence and human brain augmentation, and I know you have some experience with that life extension and, you know, Alcor and things like that, which I support those as well.

270
00:26:42,000 --> 00:26:44,000
I think it's an interesting proposition.

271
00:26:44,000 --> 00:26:53,000
But you also run into scary things like with brain implanting, a government that can tell you how to think and everyone is in lockstep.

272
00:26:53,000 --> 00:27:03,000
You become the Borg and that if that's widespread and that stands against aliens becoming grabby, that's a somewhat scary solution, don't you think?

273
00:27:03,000 --> 00:27:06,000
And we do seem to be moving towards that in some ways.

274
00:27:06,000 --> 00:27:07,000
Would you agree, though?

275
00:27:07,000 --> 00:27:22,000
So there is substantial cultural support, I think, for limiting competition and limiting expansion so that we can stay this happy integrated civilization.

276
00:27:22,000 --> 00:27:28,000
I think the problem is we have been functional in part because of a history of competition.

277
00:27:28,000 --> 00:27:35,000
And if governments repress competition too much, they may not have an alternative way to maintain functionality.

278
00:27:35,000 --> 00:27:45,000
So an overly aggressive, repressive government, you see, may fail by simply decaying away into dysfunction.

279
00:27:45,000 --> 00:27:55,000
And now the question is, can anybody at a later point in time escape from its clutches when it's decayed far enough to cause a new era of regrowth?

280
00:27:56,000 --> 00:28:05,000
The worst case scenario would be a government that's very repressive and very tightly in control and can't prevent decay.

281
00:28:05,000 --> 00:28:13,000
That is, it doesn't know how to stop itself from slowly becoming less functional, but it knows how to stop anything else from breaking away from it and competing with it.

282
00:28:13,000 --> 00:28:16,000
And, you know, the natural result of that is extinction.

283
00:28:16,000 --> 00:28:18,000
You just shrink to nothing.

284
00:28:18,000 --> 00:28:25,000
You know, at the very last minute, the very last person still has the gun to the head of the very second to last person.

285
00:28:25,000 --> 00:28:28,000
Or they just both decide to turn the lights out and that's it.

286
00:28:28,000 --> 00:28:44,000
You know, see, which eventually that's a scary thought, because eventually if indeed that, you know, then this is another hypothesis that we're simply early in the game as far as alien life goes and that there aren't very many others just because there hasn't been enough lapsed time.

287
00:28:44,000 --> 00:28:47,000
Now, I don't know that I buy that, but that's that's been put out there.

288
00:28:47,000 --> 00:28:50,000
Well, so again, the issue is how unlikely.

289
00:28:50,000 --> 00:28:56,000
So we think about the percentile rank of alien civilizations and where do we sit in that rank?

290
00:28:56,000 --> 00:29:02,000
The most obvious assumption to make to me is like roughly at 50 percent, maybe somewhere between 20 and 80 percent.

291
00:29:02,000 --> 00:29:09,000
Right. So for somewhere between 20 and 80 percent, then we have to expect that 20 percent of things have already appeared by now.

292
00:29:09,000 --> 00:29:11,000
Right. At least.

293
00:29:11,000 --> 00:29:13,000
And so we have to wonder if we could see them.

294
00:29:13,000 --> 00:29:19,000
You might say, no, we're at the one percent and I'm still going to say, OK, but now they're just a little farther away.

295
00:29:19,000 --> 00:29:22,000
We should still see them. And now you say, oh, no, I mean, one in a million.

296
00:29:22,000 --> 00:29:25,000
And I say, well, that's a crazy unlikely thing to assume.

297
00:29:25,000 --> 00:29:31,000
Right. You're assuming that we are the first out of a million, say, and now farther.

298
00:29:31,000 --> 00:29:40,000
You know, the nearest one is farther away, but I just don't think you're really winning on the assuming we're earlier in the percentile and therefore the nearest one is farther away.

299
00:29:40,000 --> 00:29:42,000
So we can see a really long way away.

300
00:29:42,000 --> 00:29:54,000
And I don't think you're really gaining that much in explanatory power in assuming we're rather early, but not crazy early, because, OK, there's fewer of them out there, but still we should see them.

301
00:29:54,000 --> 00:30:02,000
Well, and the other thing, too, and this is one of my particular problem with it, is that there isn't anything that says that someone couldn't have been here earlier.

302
00:30:02,000 --> 00:30:09,000
It seems that the universe could have been supporting a civilization in life multiple billions of years before we came along.

303
00:30:09,000 --> 00:30:12,000
We're only here because Earth happened to form at the time that it did.

304
00:30:12,000 --> 00:30:16,000
If it was eight billion years old, it still would have been possible.

305
00:30:16,000 --> 00:30:25,000
And again, that I mean, if that would have meant that we would have appeared much earlier, billions of years ago, and we should be grabby if we had.

306
00:30:25,000 --> 00:30:28,000
And we see nothing. We see absolutely nothing.

307
00:30:28,000 --> 00:30:32,000
And we look at galaxies, nearby galaxies. None of them appear to be altered.

308
00:30:32,000 --> 00:30:38,000
So the hard thing is, imagine an advanced civilization appeared many billions of years ago somewhere nearby.

309
00:30:38,000 --> 00:30:43,000
How is it that we see no appearance of its impact at all anywhere?

310
00:30:43,000 --> 00:30:46,000
It would have to have just completely died and killed itself.

311
00:30:46,000 --> 00:30:50,000
But that's really hard to do if we talk about how hard it will be to kill humans on Earth right now.

312
00:30:50,000 --> 00:30:55,000
But once we spread across thousands of star system, that gets even harder.

313
00:30:55,000 --> 00:31:00,000
So it's really hard to believe that they all killed themselves.

314
00:31:00,000 --> 00:31:02,000
And that's why there's nothing left.

315
00:31:02,000 --> 00:31:07,000
Or we could just like postulate that they just had an obsession with not having any visible impact.

316
00:31:07,000 --> 00:31:10,000
I think that's the kind of assumption you have to be drawn to.

317
00:31:10,000 --> 00:31:15,000
They showed up a while ago and they are obsessed with not messing with the natural appearance.

318
00:31:15,000 --> 00:31:20,000
Maybe. But that gets into things like zoo hypothesis and all of those sorts of things.

319
00:31:20,000 --> 00:31:21,000
What's your view on that?

320
00:31:21,000 --> 00:31:24,000
Right. But I think it has an important implication.

321
00:31:24,000 --> 00:31:32,000
If there is an alien civilization out there and it's obsessed with making no impact,

322
00:31:32,000 --> 00:31:36,000
it's probably also obsessed with preventing anyone else from having an impact, right?

323
00:31:36,000 --> 00:31:38,000
Zoo hypothesis.

324
00:31:38,000 --> 00:31:47,000
The obsession with preventing any impact would have to be an obsession with watching out for the rise of new civilizations that would then might have an impact, right?

325
00:31:48,000 --> 00:31:53,000
So an obvious prediction is not only have they been preventing themselves from spreading out and having an impact,

326
00:31:53,000 --> 00:32:03,000
they are watching for other new arrivals who might have an impact and selecting them and making sure that they don't have an impact.

327
00:32:03,000 --> 00:32:09,000
So this has the disturbing scenario that there really are aliens somewhere nearby watching us

328
00:32:09,000 --> 00:32:14,000
and not too happy with the prospect that we might soon have an impact.

329
00:32:14,000 --> 00:32:17,000
And they stop us and they stop everyone.

330
00:32:17,000 --> 00:32:21,000
And that gets into, you know, for example, abiogenesis sort of parallels us.

331
00:32:21,000 --> 00:32:24,000
We only have one type of life on Earth and it's all related.

332
00:32:24,000 --> 00:32:31,000
But if abiogenesis occurs a lot and it gets eaten every time it appears on Earth, you know, within minutes,

333
00:32:31,000 --> 00:32:34,000
then the original life form is the dominant one.

334
00:32:34,000 --> 00:32:39,000
And that's the way the universe works all the way up into technology because there's somebody out there that hides.

335
00:32:39,000 --> 00:32:40,000
So we can go a little farther.

336
00:32:40,000 --> 00:32:46,000
Look, surely an advanced civilization watching out for any other civilization that might have an impact

337
00:32:46,000 --> 00:32:52,000
has the easy option of as soon as it sees them get it all advanced, it just wipes them out.

338
00:32:52,000 --> 00:32:55,000
And clearly they didn't do that to us, right?

339
00:32:55,000 --> 00:33:04,000
So we can conclude that they would rather persuade us than force us to follow their rule of no impact.

340
00:33:04,000 --> 00:33:07,000
And then we might wonder how they would persuade us.

341
00:33:07,000 --> 00:33:14,000
Well, unless the giant near speed of light kill pineapple, you know, let's say it's shaped like a pineapple

342
00:33:14,000 --> 00:33:16,000
the size of the planet, it's going to knock Earth out.

343
00:33:16,000 --> 00:33:18,000
We wouldn't see it coming until it hits.

344
00:33:18,000 --> 00:33:21,000
But then again, they'd have to know when to send the pineapple.

345
00:33:21,000 --> 00:33:26,000
I mean, they could have seen life on Earth billions of years ago.

346
00:33:26,000 --> 00:33:27,000
And that would be a pretty rare thing.

347
00:33:27,000 --> 00:33:31,000
And then they could have seen advanced life here, you know, millions of years ago.

348
00:33:31,000 --> 00:33:34,000
So they would have had plenty of warning.

349
00:33:34,000 --> 00:33:35,000
It doesn't look terribly likely.

350
00:33:35,000 --> 00:33:38,000
But again, it doesn't look terribly likely.

351
00:33:38,000 --> 00:33:45,000
I mean, I have this explanation of the universe looking completely empty is that it is completely empty, right?

352
00:33:45,000 --> 00:33:47,000
Well, that's the simplest answer, of course.

353
00:33:47,000 --> 00:33:50,000
Well, you know, I have to ask this question again.

354
00:33:50,000 --> 00:33:54,000
And I am skeptical of this one myself, but it's going to show up in the comments.

355
00:33:54,000 --> 00:34:00,000
What is your view of the idea that we just simply don't know how to see grabby aliens?

356
00:34:00,000 --> 00:34:06,000
We don't know what an alien civilization actually looks like if it's 10,000 years or 10 million years more advanced than we are.

357
00:34:06,000 --> 00:34:12,000
And that we're just in an age where technology is very detectable, but maybe there isn't.

358
00:34:12,000 --> 00:34:20,000
So you have to start making some assumptions here about the sort of tendency to spread into different niches.

359
00:34:20,000 --> 00:34:23,000
So in the history of life on Earth, you have some kind of life.

360
00:34:23,000 --> 00:34:28,000
It will, of course, spread into whatever seems the most promising niches it can find.

361
00:34:28,000 --> 00:34:31,000
But it will also spread into the least promising niches.

362
00:34:31,000 --> 00:34:33,000
It'll just spread into all the niches, right?

363
00:34:33,000 --> 00:34:42,000
And now if you look in the history of human civilizations and, say, explorers, they just went in a lot of different directions, most of which didn't work out very well.

364
00:34:42,000 --> 00:34:43,000
It didn't always go everywhere.

365
00:34:43,000 --> 00:34:47,000
There's some scale effects, but they just went a lot of different ways.

366
00:34:47,000 --> 00:34:55,000
And, you know, the few things we see that are populated now are the rare successes where some group went somewhere and it turned out to work out well.

367
00:34:55,000 --> 00:35:03,000
So if you reason about our descendants, I think in order to say, look at the universe around us, why is it empty?

368
00:35:03,000 --> 00:35:05,000
Oh, because they went somewhere else.

369
00:35:05,000 --> 00:35:08,000
You have to say, well, OK, maybe most of them went somewhere else.

370
00:35:08,000 --> 00:35:10,000
But why didn't any of them come this way?

371
00:35:10,000 --> 00:35:16,000
Well, there are some people that say that they did and that the UFO, UAP phenomenon represents it.

372
00:35:16,000 --> 00:35:18,000
But I'm not satisfied with that either.

373
00:35:18,000 --> 00:35:20,000
Obviously. What are your views on that? Do you think that that?

374
00:35:21,000 --> 00:35:25,000
So I think there are some disturbing UFO reports.

375
00:35:25,000 --> 00:35:30,000
And I think you have to ask, what's the prior on them actually being aliens?

376
00:35:30,000 --> 00:35:32,000
So think about a murder trial.

377
00:35:32,000 --> 00:35:38,000
In a murder trial, we might ask, well, here's an accusation against a particular person murdering another one.

378
00:35:38,000 --> 00:35:40,000
We're going to be asked if we believe it.

379
00:35:40,000 --> 00:35:43,000
But we have to start out with what's the prior for this being true.

380
00:35:43,000 --> 00:35:46,000
If the prior was low enough, we would just say, no, I'm not going to listen to your evidence.

381
00:35:46,000 --> 00:35:47,000
That's crazy.

382
00:35:47,000 --> 00:35:53,000
And in a typical murder trial, the prior is, well, there's a one in a thousand chance any one person is murdered

383
00:35:53,000 --> 00:35:57,000
and say roughly a thousand people, associates who might have done it.

384
00:35:57,000 --> 00:36:01,000
So there's maybe a one in a million prior that this person could have murdered that person.

385
00:36:01,000 --> 00:36:05,000
And with a one in a million prior, that's high enough that we're willing to listen to the evidence.

386
00:36:05,000 --> 00:36:07,000
We don't say, oh, that's crazy unlikely.

387
00:36:07,000 --> 00:36:11,000
Just don't talk to me about it because that couldn't have happened.

388
00:36:11,000 --> 00:36:16,000
We go, well, commonly available evidence is often able to overcome a one in a million prior.

389
00:36:16,000 --> 00:36:19,000
And so for UFOs, you have to ask, well, what's the prior here?

390
00:36:19,000 --> 00:36:25,000
If it was, you know, if the prior was like one in a quadrillion or something, you might well say, no, that's so crazy.

391
00:36:25,000 --> 00:36:27,000
I'm not going to look at your evidence.

392
00:36:27,000 --> 00:36:28,000
It's far more likely.

393
00:36:28,000 --> 00:36:33,000
Your evidence was just created by accidents, even if they're very unlikely accidents than actual aliens.

394
00:36:33,000 --> 00:36:39,000
Right. Well, I thought it should be my job to sit down and figure out what would a reasonable prior be

395
00:36:39,000 --> 00:36:41,000
because I had done this grabby aliens work.

396
00:36:41,000 --> 00:36:47,000
And I sat down and I did that and I thought a plausible prior to say roughly one in a thousand, one in ten thousand.

397
00:36:47,000 --> 00:36:51,000
And at that prior, I say, well, you do got to look at the evidence.

398
00:36:51,000 --> 00:36:54,000
You can't just wave it away saying that's crazy unlikely.

399
00:36:54,000 --> 00:36:57,000
I'm not an expert in the particular evidence.

400
00:36:57,000 --> 00:37:01,000
I can just look at enough and I can say, well, I can't just easily dismiss that.

401
00:37:01,000 --> 00:37:02,000
That's kind of disturbing.

402
00:37:02,000 --> 00:37:05,000
And somebody else should look at that evidence.

403
00:37:05,000 --> 00:37:08,000
So that's my position on the UFOs is it's not crazy.

404
00:37:08,000 --> 00:37:11,000
It certainly might be wrong, but it's not crazy.

405
00:37:11,000 --> 00:37:12,000
And somebody should be looking at that.

406
00:37:12,000 --> 00:37:16,000
And then my job is to come up with the most plausible scenario that could make sense of it.

407
00:37:16,000 --> 00:37:22,000
And again, I think the most plausible scenario is we have panspermia siblings.

408
00:37:22,000 --> 00:37:24,000
Life on Earth didn't start on Earth.

409
00:37:24,000 --> 00:37:25,000
It started somewhere else.

410
00:37:25,000 --> 00:37:33,000
And then when it spread from there to here, it also spread to all the other stars in our stellar nursery, say roughly ten thousand stars.

411
00:37:34,000 --> 00:37:42,000
Those spread across the galaxy in a ring after the initial birthing years in this initial nebula.

412
00:37:42,000 --> 00:37:50,000
And then there were ten thousand other stars that were seated with life that one of them could have gotten to an advanced level before us.

413
00:37:50,000 --> 00:38:02,000
And then if it did and it had one of these rules against expansion, then it would seek searches out its other siblings to get us to not expand also.

414
00:38:02,000 --> 00:38:08,000
I don't think that's been discussed enough because the sun's birth cluster, there was a lot of stars in close proximity.

415
00:38:08,000 --> 00:38:16,000
And we're starting to see chemically similar stars to the sun that appear to have been your at least candidates to have been in the same birth cluster.

416
00:38:16,000 --> 00:38:23,000
And you would assume, you know, you're not going to really end up with, you know, planets around just one of them.

417
00:38:23,000 --> 00:38:25,000
You know, they're all probably going to have something.

418
00:38:25,000 --> 00:38:38,000
And if panspermia is occurring like that and life didn't originally come from here, then that's an interesting thought because that means that there's related life scattered through the Milky Way at this point.

419
00:38:38,000 --> 00:38:43,000
And but again, none of them appear grabby if they're there, you know.

420
00:38:43,000 --> 00:38:50,000
Right. So either. I mean, most likely most of them just didn't ever reach an advanced level like we have.

421
00:38:50,000 --> 00:38:57,000
But if one did, it would have just re remade the galaxy unless it had some rule against that.

422
00:38:57,000 --> 00:39:03,000
That's interesting. I mean, yeah, I mean, for all we know, some of those planets may never have even hit photosynthesis.

423
00:39:03,000 --> 00:39:08,000
The conditions may not have been right and it's all just anaerobic microbes still.

424
00:39:08,000 --> 00:39:14,000
But the hard steps model says that, look, it's just really hard and they probably didn't get for the first two steps.

425
00:39:14,000 --> 00:39:16,000
Yeah. And we're at step six, say, right.

426
00:39:16,000 --> 00:39:20,000
Right. And now, well, say, step six is supposed to be the hardest, right?

427
00:39:20,000 --> 00:39:23,000
Well, all the steps would be roughly similar hard.

428
00:39:23,000 --> 00:39:25,000
Roughly similar hard. I see.

429
00:39:25,000 --> 00:39:31,000
But we would just see them roughly equally spaced in time and we wouldn't really be able to tell which were the harder ones.

430
00:39:31,000 --> 00:39:33,000
We would just know all of them were hard.

431
00:39:33,000 --> 00:39:42,000
Now, what is the solution to the Fermi paradox and the solution to why we see grabby aliens that disturbs you the most?

432
00:39:42,000 --> 00:39:45,000
And for me, that that would be the zoo hypothesis.

433
00:39:45,000 --> 00:39:50,000
But which one do you look at and say, hmm, that's not a very nice one.

434
00:39:50,000 --> 00:39:54,000
I mean, I look out and I say there is definitely a huge, great filter.

435
00:39:54,000 --> 00:39:57,000
And that's the question is, is it behind us or ahead of us?

436
00:39:57,000 --> 00:40:02,000
And clearly, it's more disturbing to think of a filter ahead of us than behind us.

437
00:40:02,000 --> 00:40:06,000
However difficult the steps were behind us, we're past them.

438
00:40:06,000 --> 00:40:08,000
Not so disturbing now that we're past them, right?

439
00:40:08,000 --> 00:40:13,000
Right. The disturbing scenario is that we face a filter in front of us.

440
00:40:13,000 --> 00:40:23,000
Again, I don't think that can really be a try, try filter, but it could well be some other kinds of filters killing ourselves or most plausibly deciding not to expand.

441
00:40:23,000 --> 00:40:26,000
So that's the scenario that disturbs me.

442
00:40:26,000 --> 00:40:35,000
So we will decide not to expand, especially in light that that we're right now about to remove a great filter, which is asteroid impacts.

443
00:40:35,000 --> 00:40:37,000
We're already able to deflect them.

444
00:40:37,000 --> 00:40:43,000
And in a hundred years, it'll be something that we just probably have an innate ability to do with our technology.

445
00:40:43,000 --> 00:40:46,000
And that was obviously the big filter for the dinosaurs.

446
00:40:46,000 --> 00:40:50,000
But if there still lies an unknown, great filter in the future, something we don't know about.

447
00:40:50,000 --> 00:40:58,000
I think an outside astrophysical force just isn't very plausible as a future filter because we can see the rate in the past is low.

448
00:40:58,000 --> 00:41:00,000
They don't happen every hundred years.

449
00:41:00,000 --> 00:41:05,000
The rate at which enormous things kill off huge fractions of life on Earth is rare.

450
00:41:05,000 --> 00:41:08,000
And so it looks like we say how long should it take us to become gravity?

451
00:41:08,000 --> 00:41:10,000
I'd say a thousand years.

452
00:41:10,000 --> 00:41:16,000
And I'd say the fraction of outside impacts that happen in a typical thousand year period is just really low.

453
00:41:16,000 --> 00:41:21,000
So that means that's just not a concern in our near future.

454
00:41:21,000 --> 00:41:24,000
But things we could do it ourselves.

455
00:41:24,000 --> 00:41:27,000
Those are problems.

456
00:41:27,000 --> 00:41:29,000
Or something that could really slow us down.

457
00:41:29,000 --> 00:41:31,000
Well, things that we could do to ourselves.

458
00:41:31,000 --> 00:41:34,000
What are your thoughts on artificial intelligence?

459
00:41:34,000 --> 00:41:37,000
And does it actually pose an existential threat to humans?

460
00:41:37,000 --> 00:41:48,000
So the artificial intelligence of any sort just can't be an explanation for the great filter because if unfriendly AIs kill off the humans,

461
00:41:48,000 --> 00:41:52,000
they would still plausibly go on to expand and make a visible impact.

462
00:41:52,000 --> 00:41:58,000
So it could be a bad outcome, but it's not plausibly an explanation for the great filter.

463
00:41:58,000 --> 00:42:01,000
It's not you just end up with a machine civilization.

464
00:42:01,000 --> 00:42:03,000
Right. And then it's still visible.

465
00:42:03,000 --> 00:42:07,000
And so it's still its absence is still something to be explained.

466
00:42:07,000 --> 00:42:12,000
Now, I said my biggest concern was that we would choose not to expand.

467
00:42:12,000 --> 00:42:21,000
But a related concern is that we are now going to lose innovation in the next few decades and then not get it back for several centuries.

468
00:42:21,000 --> 00:42:25,000
So we've been on an innovation tear for a few centuries now.

469
00:42:25,000 --> 00:42:31,000
And that's gone along with increasing economy because of an increasing population.

470
00:42:31,000 --> 00:42:39,000
And straightforward economic theory suggests that as population declines, innovation would then decline in more than proportion.

471
00:42:39,000 --> 00:42:43,000
And so innovation will basically grind to a halt.

472
00:42:43,000 --> 00:42:47,000
So population expected to peak in maybe three decades or so.

473
00:42:47,000 --> 00:42:50,000
And then innovation will be declining.

474
00:42:50,000 --> 00:42:58,000
And if we let population falls by a factor of 10 or 20, you'd see fall in the innovation rate by that scale.

475
00:42:58,000 --> 00:43:03,000
And then we would just have several centuries of not much innovation.

476
00:43:03,000 --> 00:43:09,000
And then the revival eventually of innovation would come from, say, groups like the Amish or Heretim,

477
00:43:09,000 --> 00:43:13,000
who would take over the world by continuing to double their population every 20 years.

478
00:43:13,000 --> 00:43:19,000
But they probably wouldn't pick up innovation again for a while because they're not really that pro innovation.

479
00:43:19,000 --> 00:43:24,000
And so we could just face a long delay here and we might lose the habit of innovation.

480
00:43:24,000 --> 00:43:27,000
And maybe that would take a lot longer to pick up again.

481
00:43:27,000 --> 00:43:31,000
And now you've got a lot longer history where more things can go wrong.

482
00:43:31,000 --> 00:43:39,000
And you can also imagine situations where innovation is stopped not just by population growth of groups that are anti innovation,

483
00:43:39,000 --> 00:43:45,000
but the population at large eventually having a reason to become anti innovation themselves.

484
00:43:45,000 --> 00:43:52,000
In other words, it favors stagnation because, you know, too many people built too many androids.

485
00:43:52,000 --> 00:43:53,000
We don't need them.

486
00:43:53,000 --> 00:43:59,000
They burst in innovation in the last few centuries is a historically very unusual thing.

487
00:43:59,000 --> 00:44:02,000
And you have to wonder how fragile it is.

488
00:44:02,000 --> 00:44:10,000
So when the first nations on Earth started to be very innovative with the Industrial Revolution, Holland and then England,

489
00:44:10,000 --> 00:44:13,000
as soon as they started to grow and become powerful,

490
00:44:13,000 --> 00:44:18,000
then other nations on the earth felt like they needed to try to copy them or they to be left behind.

491
00:44:18,000 --> 00:44:24,000
So they tried to copy the innovation promoting attitudes of Western Europe,

492
00:44:24,000 --> 00:44:30,000
including their liberality, in order to not be left behind.

493
00:44:30,000 --> 00:44:40,000
And so if innovation goes away, there will no longer be a need to be open and innovative and liberal in order to not be left behind.

494
00:44:40,000 --> 00:44:46,000
So more plausibly, they would revert to their more natural authoritarian styles.

495
00:44:46,000 --> 00:44:56,000
If we have a much more authoritarian and stable conservative regimes and societies that might take a long time for them to revive the habit of innovation.

496
00:44:56,000 --> 00:44:59,000
The what comes to mind is planet of the apes.

497
00:44:59,000 --> 00:45:05,000
You uplift the apes, the apes take over and that stagnates innovation because they just aren't very innovative.

498
00:45:05,000 --> 00:45:11,000
But again, like even a factor of a hundred slower innovation is still plenty of time on cosmological scales.

499
00:45:11,000 --> 00:45:14,000
Plenty. It's plenty. Any innovation.

500
00:45:14,000 --> 00:45:23,000
And you would have to you would have to stagnate to such an extraordinary degree that I just don't see it.

501
00:45:23,000 --> 00:45:30,000
I just don't see it. I mean, even if we had a nuclear war, we would have our technology back within a matter of years and decades.

502
00:45:30,000 --> 00:45:33,000
You know, we wouldn't we know about all this stuff.

503
00:45:33,000 --> 00:45:38,000
And I don't think that we would be able to to actually not rebuild it.

504
00:45:38,000 --> 00:45:48,000
You know, yeah, that's why I'm more worried about intentionally choosing to forbid innovation or expansion or colonization.

505
00:45:48,000 --> 00:45:55,000
I can more plausibly see a mid-level power hasn't collapsed completely, still has abilities of surveillance and repression.

506
00:45:55,000 --> 00:46:02,000
And it chooses not to allow innovation or expansion because of agendas.

507
00:46:02,000 --> 00:46:05,000
And now this needs to be global. It needs to be a civilization wide government.

508
00:46:05,000 --> 00:46:07,000
It has to be global. It is. It does.

509
00:46:07,000 --> 00:46:11,000
And I don't know. I suppose a world government eventually will happen.

510
00:46:11,000 --> 00:46:17,000
But our attempts at it, like with the UN and things like that, have not been successful so far.

511
00:46:17,000 --> 00:46:21,000
But I don't know how fractious alien civilizations are. Just that we're fractious.

512
00:46:21,000 --> 00:46:24,000
Well, we're closer than you think, actually. Sorry.

513
00:46:24,000 --> 00:46:30,000
So what we have is a pretty unified world community with norms.

514
00:46:30,000 --> 00:46:38,000
So if you think about COVID, the very beginning of COVID, the usual public health experts said the usual things about what to do in a pandemic.

515
00:46:38,000 --> 00:46:41,000
And then elites around the world talked about it for a month.

516
00:46:41,000 --> 00:46:48,000
So at the end of that month, they came up with a different answer from what the usual experts had said.

517
00:46:48,000 --> 00:46:53,000
And then the entire world basically did it this new elite way with very few exceptions.

518
00:46:53,000 --> 00:47:04,000
If you look at regulation in many different areas, similarly, we see very strong convergence around the world in regulation of banking and nuclear power and medical experiments and airlines and all sorts of things.

519
00:47:04,000 --> 00:47:09,000
So we actually have a pretty strong shared consensus around the world on many things.

520
00:47:09,000 --> 00:47:14,000
For example, there's only one country in the world that now allows organ sales, which happens to be Iran.

521
00:47:14,000 --> 00:47:19,000
And bioethicists are getting together all the time trying to talk about how are we going to get Iran to stop this.

522
00:47:19,000 --> 00:47:23,000
And that's true for a lot of things where there's no exceptions or one exception.

523
00:47:23,000 --> 00:47:27,000
And the world is working hard to prevent and stop those exceptions.

524
00:47:27,000 --> 00:47:35,000
So even though we don't have a central world government, we do, in fact, have a world mob that gets its way.

525
00:47:35,000 --> 00:47:43,000
So we're a lot closer than you might think to a world that agrees on some things like agreeing that certain kinds of technologies just aren't going to be allowed.

526
00:47:43,000 --> 00:47:46,000
Well, cloning. We seem to have more or less.

527
00:47:46,000 --> 00:47:49,000
We haven't seen human cloning, right? And nuclear, really.

528
00:47:49,000 --> 00:47:53,000
We've kind of we really, you know, cut the legs out of under nuclear.

529
00:47:53,000 --> 00:48:03,000
We have prevented genetic engineering of many sorts, and we could well want to prevent other kinds of technology that seem threatening, including ways that we might change ourselves.

530
00:48:03,000 --> 00:48:09,000
And then later on, we will realize interstellar colonization is this uber change it.

531
00:48:09,000 --> 00:48:14,000
Once you allow interstellar colonization, you can't prevent a huge space of change.

532
00:48:14,000 --> 00:48:17,000
Very well possible. Very well possible.

533
00:48:17,000 --> 00:48:31,000
And it's I think would actually really we might confront this earlier than we think we might, because if you get a Mars colony and nobody controls that Mars colony, except those colonists, then you have a potential enemy.

534
00:48:31,000 --> 00:48:36,000
And I don't know if you could have an interplanetary war very easily, but you could have conflict for sure.

535
00:48:36,000 --> 00:48:50,000
Well, conflict within the solar system is not such an issue that I say, you know, we can have a global civilization now on Earth because we can communicate easily and travel easily and lob bombs at each other easily.

536
00:48:50,000 --> 00:48:52,000
And that remains true within the solar system.

537
00:48:52,000 --> 00:48:58,000
The solar system is small enough that we will be have cheap, fast travel, communication and bomb lobby.

538
00:48:58,000 --> 00:49:04,000
But once you leave this lawsuit for another star, suddenly those things are no longer so cheap and easy.

539
00:49:04,000 --> 00:49:07,000
A time delay for lobbing a bomb is just way too long.

540
00:49:07,000 --> 00:49:10,000
Oh, yeah. Well, I don't think I don't think it would come to that.

541
00:49:10,000 --> 00:49:12,000
I think it would come to just power place.

542
00:49:12,000 --> 00:49:14,000
We want to control that Mars colony.

543
00:49:14,000 --> 00:49:18,000
The Mars colony says, no, we're cutting off all contact with you.

544
00:49:18,000 --> 00:49:22,000
And then Earth says, well, you're not getting any more goods from Earth.

545
00:49:22,000 --> 00:49:24,000
Good luck on self sufficiency on that rock out there.

546
00:49:24,000 --> 00:49:26,000
And I could see things like that happening.

547
00:49:26,000 --> 00:49:29,000
But at the same time happening on Earth right now, that's happening on Earth right now.

548
00:49:29,000 --> 00:49:35,000
Trade, trade restrictions as a way to, you know, try to bring some nations to heel.

549
00:49:35,000 --> 00:49:37,000
Yeah, absolutely.

550
00:49:37,000 --> 00:49:39,000
Consensus sanctioning. Yeah.

551
00:49:39,000 --> 00:49:49,000
Now, there is one encouraging thing, though, regarding becoming grabby and not putting the kibosh on it with a government is that they don't appear to be doing it.

552
00:49:49,000 --> 00:49:51,000
Everybody seems to want to go to space.

553
00:49:51,000 --> 00:50:01,000
And we, you know, we have China, we have India, we have the United States, we have Russia, we have, you know, all of these countries and everybody in some way or another is involved.

554
00:50:01,000 --> 00:50:07,000
And it doesn't look like there isn't it doesn't look at this point to be an anti space movement forming up.

555
00:50:07,000 --> 00:50:09,000
But that could change. Right.

556
00:50:09,000 --> 00:50:14,000
But again, it's only leaving the solar system that's the threat, not going into space.

557
00:50:14,000 --> 00:50:15,000
True, true.

558
00:50:15,000 --> 00:50:19,000
But at the same time, leaving the solar system for us right now would be hard.

559
00:50:19,000 --> 00:50:21,000
Maybe little micro probes.

560
00:50:21,000 --> 00:50:29,000
Right. So the question is, will there be opposition that forms once people realize that the implications of somebody leaving the solar system and it becomes an actual option.

561
00:50:29,000 --> 00:50:31,000
Going to be an interesting future indeed.

562
00:50:31,000 --> 00:50:32,000
I don't know.

563
00:50:32,000 --> 00:50:44,000
That's a seems to be a fertile territory for the sci fi authors to imagine is, you know, inter interstellar rivalries that could form up between people that originated on the same planet.

564
00:50:44,000 --> 00:50:48,000
And that could be a reason to pull back and not allow it.

565
00:50:48,000 --> 00:50:50,000
Very interesting ideas.

566
00:50:50,000 --> 00:50:55,000
Thanks Robin for joining us today and I look forward to perhaps another conversation in the future.

567
00:50:55,000 --> 00:51:03,000
As the great debate over the Fermi paradox continues, which could end on a dime if somebody sees something.

568
00:51:03,000 --> 00:51:05,000
But we haven't yet.

569
00:51:05,000 --> 00:51:06,000
Not today.

570
00:51:14,000 --> 00:51:16,000
Yeah.

571
00:51:44,000 --> 00:51:45,000
Yeah.

572
00:52:44,000 --> 00:52:46,000
Yeah.

